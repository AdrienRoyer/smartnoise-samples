{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.exponential(1.0)\n",
    "\n",
    "Y = np.array([1,2,3,4,5])\n",
    "Y.reshape((len(Y),1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.88233104 -10.46311058  -9.37634218  -2.23357951   4.00328849\n",
      "  15.61315814  -6.41310138  -2.5082783   -1.5854718   -4.65818988]\n",
      "[-1.36228371  2.07106066  1.85594649  0.44211314 -0.79240807 -3.09045741\n",
      "  1.26940472  0.49648682  0.31382716  0.92203879]\n",
      "4.771842267911936\n",
      "[-0.28548381  0.434017    0.3889371   0.09265041 -0.16605915 -0.6476445\n",
      "  0.26601984  0.1040451   0.06576646  0.19322491]\n",
      "[22.57238815]\n",
      "[ -6.88233104  10.46311058   9.37634218   2.23357951  -4.00328849\n",
      " -15.61315814   6.41310138   2.5082783    1.5854718    4.65818988]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "Length = 10\n",
    "beta = .5\n",
    "\n",
    "e1 = np.random.normal(size=Length)\n",
    "e2 = -1 * preprocessing.normalize([e1], norm='l2')\n",
    "noise = np.random.gamma(Length, 1/beta)\n",
    "e3 = e2 * noise\n",
    "# e3 = (1.0 / beta) * np.exp(e2)\n",
    "print(e3[0])\n",
    "\n",
    "r1 =e1#standard normal distribution\n",
    "print(r1)\n",
    "n1 = np.linalg.norm( r1, 2 )#get the norm of this random vector\n",
    "print(n1)\n",
    "r2 = r1 / n1#the norm of r2 is 1\n",
    "print(r2)\n",
    "normn = np.random.gamma( Length, 1/beta, 1 )#Generate the norm of noise according to gamma distribution\n",
    "print(normn)\n",
    "res = r2 * noise\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from opendp.whitenoise.synthesizers.base import SDGYMBaseSynthesizer\n",
    "\n",
    "class MWEMSynthesizerTarget(SDGYMBaseSynthesizer):\n",
    "    \"\"\"\n",
    "    N-Dimensional numpy implementation of MWEM. \n",
    "    (http://users.cms.caltech.edu/~katrina/papers/mwem-nips.pdf)\n",
    "\n",
    "    From the paper:\n",
    "    \"[MWEM is] a broadly applicable, simple, and easy-to-implement algorithm, capable of\n",
    "    substantially improving the performance of linear queries on many realistic datasets...\n",
    "    (circa 2012)...MWEM matches the best known and nearly\n",
    "    optimal theoretical accuracy guarantees for differentially private \n",
    "    data analysis with linear queries.\"\n",
    "\n",
    "    Linear queries used for sampling in this implementation are\n",
    "    random contiguous slices of the n-dimensional numpy array. \n",
    "    \"\"\"\n",
    "    def __init__(self, Q_count=400, epsilon=3.0, iterations=30, mult_weights_iterations=20, splits = [], split_factor=None, max_bin_count=500):\n",
    "        self.Q_count = Q_count\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = iterations\n",
    "        self.mult_weights_iterations = mult_weights_iterations\n",
    "        self.synthetic_data = None\n",
    "        self.data_bins = None\n",
    "        self.real_data = None\n",
    "        self.splits = splits\n",
    "        self.split_factor = split_factor\n",
    "        self.max_bin_count = max_bin_count\n",
    "        self.mins_maxes = {}\n",
    "        self.scale = {}\n",
    "\n",
    "        # For fitting\n",
    "        self.seed_model = None\n",
    "        self.labels = None\n",
    "        self.label_column = None\n",
    "        \n",
    "    def fit(self, data, model, column):\n",
    "        \"\"\"\n",
    "        Creates a synthetic histogram distribution, based on the original data.\n",
    "        Follows sdgym schema to be compatible with their benchmark system.\n",
    "\n",
    "        :param data: Dataset to use as basis for synthetic data\n",
    "        :type data: np.ndarray\n",
    "        :return: synthetic data, real data histograms\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        if isinstance(data, np.ndarray):\n",
    "            # We remove the label column from the general data\n",
    "            p_c = np.arange(data.shape[1])\n",
    "            p_c = np.delete(p_c, column)\n",
    "            self.data = data[:, p_c].copy()\n",
    "            self.labels = data[:,[column]].copy()\n",
    "            self.label_column = column\n",
    "\n",
    "            # Save the model for later\n",
    "            self.seed_model = model\n",
    "        else:\n",
    "            raise ValueError(\"Data must be a numpy array.\")\n",
    "\n",
    "        if self.split_factor != None and self.splits == []:\n",
    "            self.splits = self._generate_splits(self.data.T.shape[0], self.split_factor)\n",
    "\n",
    "        self.splits = np.array(self.splits)\n",
    "        if self.splits.size == 0:\n",
    "            self.histograms = self._histogram_from_data_attributes(self.data, [np.arange(self.data.shape[1])])\n",
    "        else:\n",
    "            self.histograms = self._histogram_from_data_attributes(self.data, self.splits)\n",
    "        \n",
    "        self.Qs = []\n",
    "        for h in self.histograms:\n",
    "            # h[1] is dimensions for each histogram\n",
    "            self.Qs.append(self._compose_arbitrary_slices(self.Q_count, h[1]))\n",
    "\n",
    "        # Run the algorithm\n",
    "        self.synthetic_histograms = self.mwem()\n",
    "\n",
    "    def sample(self, samples):\n",
    "        \"\"\"\n",
    "        Creates samples from the histogram data.\n",
    "        Follows sdgym schema to be compatible with their benchmark system.\n",
    "\n",
    "        :param samples: Number of samples to generate\n",
    "        :type samples: int\n",
    "        :return: N samples\n",
    "        :rtype: list(np.ndarray)\n",
    "        \"\"\"\n",
    "        synthesized_columns = ()\n",
    "        first = True\n",
    "        for fake, _ , split in self.synthetic_histograms:\n",
    "            s = []\n",
    "            fake_indices = np.arange(len(np.ravel(fake)))\n",
    "            fake_distribution = np.ravel(fake)\n",
    "            norm = np.sum(fake)\n",
    "\n",
    "            for _ in range(samples):\n",
    "                s.append(np.random.choice(fake_indices, p=(fake_distribution/norm)))\n",
    "\n",
    "            s_unraveled = []\n",
    "            for ind in s:\n",
    "                s_unraveled.append(np.unravel_index(ind,fake.shape))\n",
    "            \n",
    "            # Here we make scale adjustments to match the original\n",
    "            # data\n",
    "            np_unraveled = np.array(s_unraveled)\n",
    "            \n",
    "            for i in range(np_unraveled.shape[-1]):\n",
    "                min_c, max_c = self.mins_maxes[str(split[i])]\n",
    "                # TODO: Deal with the 0 edge case when scaling\n",
    "                # i.e. scale factor * 0th bin is 0, \n",
    "                # but should still scale appropriately\n",
    "                np_unraveled[:,i] = np_unraveled[:,i] * self.scale[str(split[i])]\n",
    "                np_unraveled[:,i] = np_unraveled[:,i] + min_c\n",
    "            \n",
    "            if first:\n",
    "                synthesized_columns = np_unraveled\n",
    "                first = False\n",
    "            else:\n",
    "                synthesized_columns = np.hstack((synthesized_columns, np_unraveled))\n",
    "        \n",
    "        # Recombine the independent distributions into a single dataset\n",
    "        combined = synthesized_columns\n",
    "        # Reorder the columns to mirror their original order\n",
    "        r = self._reorder(self.splits)\n",
    "        \n",
    "        x_data = combined[:,r]\n",
    "\n",
    "        # Now we need to read the labels with proper predictions\n",
    "        Y = np.array(self.seed_model.predict(x_data))\n",
    "        \n",
    "        # We have an odd distribution over the single properly predicted column\n",
    "        # \n",
    "        \n",
    "        # Turn it into 2 dims\n",
    "        Y_d = Y.reshape((len(Y),1))\n",
    "\n",
    "        return np.hstack((x_data, Y_d))\n",
    "\n",
    "    def mwem(self):\n",
    "        \"\"\"\n",
    "        Runner for the mwem algorithm. \n",
    "\n",
    "        Initializes the synthetic histogram, and updates it\n",
    "        for self.iterations using the exponential mechanism and\n",
    "        multiplicative weights. Draws from the initialized query store\n",
    "        for measurements.\n",
    "        :return: A, self.histogram - A is the synthetic data histogram, self.histogram is original histo\n",
    "        :rtype: np.ndarray, np.ndarray\n",
    "        \"\"\"\n",
    "        As = []\n",
    "\n",
    "        for i,h in enumerate(self.histograms):\n",
    "            hist = h[0]\n",
    "            dimensions = h[1]\n",
    "            split = h[3]\n",
    "            Q = self.Qs[i]\n",
    "            A = self._initialize_A(hist, dimensions)\n",
    "            measurements = {}\n",
    "            first = True\n",
    "\n",
    "            for i in range(self.iterations):\n",
    "                if first:\n",
    "                    print('Initializing iteration with columns ' + str(split))\n",
    "                    start = time.time()\n",
    "\n",
    "                qi = self._exponential_mechanism(hist, A, Q, (self.epsilon / (2*self.iterations)))\n",
    "\n",
    "                # Make sure we get a different query to measure:\n",
    "                while(qi in measurements):\n",
    "                    qi = self._exponential_mechanism(hist, A, Q, (self.epsilon / (2*self.iterations)))\n",
    "\n",
    "                # NOTE: Add laplace noise here with budget\n",
    "                evals = self._evaluate(Q[qi], hist)\n",
    "                lap = self._laplace((2*self.iterations)/(self.epsilon*len(dimensions)))\n",
    "                measurements[qi] = evals + lap\n",
    "\n",
    "                # Improve approximation with Multiplicative Weights\n",
    "                A = self._multiplicative_weights(A, Q, measurements, hist, self.mult_weights_iterations)\n",
    "                \n",
    "                if first:\n",
    "                    end = time.time()\n",
    "                    # Lower bound time estimate on next synthetic histogram\n",
    "                    # Introduces arbitrary time factor, to reduce expectations : )\n",
    "                    arbitrary_time_factor = self.iterations % 10 # Seems to slow down proportionally every ~10 iterations\n",
    "                    t = str(round((end-start) * self.iterations * arbitrary_time_factor,3))\n",
    "                    print('Estimate duration till completion: ' + t + ' seconds (lower bound)')\n",
    "                    first = False\n",
    "\n",
    "            As.append((A,hist,split))\n",
    "\n",
    "        return As\n",
    "    \n",
    "    def _initialize_A(self, histogram, dimensions):\n",
    "        \"\"\"\n",
    "        Initializes a uniform distribution histogram from\n",
    "        the given histogram with dimensions\n",
    "\n",
    "        :param histogram: Reference histogram\n",
    "        :type histogram: np.ndarray\n",
    "        :param dimensions: Reference dimensions\n",
    "        :type dimensions: np.ndarray\n",
    "        :return: New histogram, uniformly distributed according to\n",
    "        reference histogram\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        # NOTE: Could actually use a distribution from real data with some budget,\n",
    "        # as opposed to using this uniform dist (would take epsilon as argument,\n",
    "        # and detract from it)\n",
    "        n = np.sum(histogram)\n",
    "        value = n/np.prod(dimensions)\n",
    "        A = np.zeros_like(histogram)\n",
    "        A += value\n",
    "        return A\n",
    "\n",
    "    def _histogram_from_data_attributes(self, data, splits=[]):\n",
    "        \"\"\"\n",
    "        Create a histogram from given data\n",
    "\n",
    "        :param data: Reference histogram\n",
    "        :type data: np.ndarray\n",
    "        :return: Histogram over given data, dimensions, \n",
    "        bins created (output of np.histogramdd)\n",
    "        :rtype: np.ndarray, np.shape, np.ndarray\n",
    "        \"\"\"\n",
    "        histograms = []\n",
    "        for split in splits:\n",
    "            split_data = data[:, split]\n",
    "            mins_data = []\n",
    "            maxs_data = []\n",
    "            dims_sizes = []\n",
    "\n",
    "            # Transpose for column wise iteration\n",
    "            for i, column in enumerate(split_data.T):\n",
    "                min_c = min(column) ; max_c = max(column) \n",
    "                mins_data.append(min_c)\n",
    "                maxs_data.append(max_c)\n",
    "                # Dimension size (number of bins)\n",
    "                bin_count = max_c-min_c+1\n",
    "                # Here we track the min and max for the column,\n",
    "                # for sampling\n",
    "                self.mins_maxes[str(split[i])] = (min_c, max_c)\n",
    "                if bin_count > self.max_bin_count:\n",
    "                    # Note the limitations of MWEM here, specifically in the case of continuous data.\n",
    "                    warnings.warn('Bin count ' + str(bin_count) + ' in column: ' + str(split[i]) + \\\n",
    "                                  ' exceeds max_bin_count, defaulting to: ' + str(self.max_bin_count)+\\\n",
    "                                  '. Is this a continuous variable?', Warning)\n",
    "                    bin_count = self.max_bin_count\n",
    "                    # We track a scaling factor per column, for sampling\n",
    "                    self.scale[str(split[i])] = (max_c-min_c+1)/self.max_bin_count\n",
    "                else:\n",
    "                    self.scale[str(split[i])] = 1\n",
    "                dims_sizes.append(bin_count)\n",
    "            \n",
    "            # Produce an N,D dimensional histogram, where\n",
    "            # we pre-specify the bin sizes to correspond with \n",
    "            # our ranges above\n",
    "            histogram, bins = np.histogramdd(split_data, bins=dims_sizes)\n",
    "            # Return histogram, dimensions\n",
    "            histograms.append((histogram, dims_sizes, bins, split))\n",
    "\n",
    "        return histograms\n",
    "    \n",
    "    def _exponential_mechanism(self, hist, A, Q, eps):\n",
    "        \"\"\"\n",
    "        Refer to paper for in depth description of\n",
    "        Exponential Mechanism.\n",
    "\n",
    "        Parametrized with epsilon value epsilon/(2 * iterations)\n",
    "\n",
    "        :param hist: Basis histogram\n",
    "        :type hist: np.ndarray\n",
    "        :param A: Synthetic histogram\n",
    "        :type A: np.ndarray\n",
    "        :param Q: Queries to draw from\n",
    "        :type Q: list\n",
    "        :param eps: Budget\n",
    "        :type eps: float\n",
    "        :return: # of errors\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        errors = [abs(self._evaluate(Q[i], hist)-self._evaluate(Q[i], A)) * (eps/2.0) for i in range(len(Q))]\n",
    "        maxi = max(errors)\n",
    "        errors = [math.exp(errors[i] - maxi) for i in range(len(errors))]\n",
    "        r = random.random()\n",
    "        e_s = np.sum(errors)\n",
    "        c = 0\n",
    "        for i in range(len(errors)):\n",
    "            c += errors[i]\n",
    "            if c > r * e_s:\n",
    "                return i\n",
    "        return len(errors) - 1\n",
    "    \n",
    "    def _multiplicative_weights(self, A, Q, m, hist, iterate):\n",
    "        \"\"\"\n",
    "        Multiplicative weights update algorithm,\n",
    "        used to boost the synthetic data accuracy given measurements m.\n",
    "\n",
    "        Run for iterate times\n",
    "\n",
    "        \n",
    "        :param A: Synthetic histogram\n",
    "        :type A: np.ndarray\n",
    "        :param Q: Queries to draw from\n",
    "        :type Q: list\n",
    "        :param m: Measurements taken from real data for each qi query\n",
    "        :type m: dict\n",
    "        :param hist: Basis histogram\n",
    "        :type hist: np.ndarray\n",
    "        :param iterate: Number of iterations to run mult weights\n",
    "        :type iterate: iterate\n",
    "        :return: A\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        sum_A = np.sum(A)\n",
    "\n",
    "        for _ in range(iterate):\n",
    "            for qi in m:\n",
    "                error = m[qi] - self._evaluate(Q[qi], A)\n",
    "\n",
    "                # Perform the weights update\n",
    "                query_update = self._binary_replace_in_place_slice(np.zeros_like(A.copy()), Q[qi])\n",
    "                \n",
    "                # Apply the update\n",
    "                A_multiplier = np.exp(query_update * error/(2.0 * sum_A))\n",
    "                A_multiplier[A_multiplier == 0.0] = 1.0\n",
    "                A = A * A_multiplier\n",
    "\n",
    "                # Normalize again\n",
    "                count_A = np.sum(A)\n",
    "                A = A * (sum_A/count_A)\n",
    "        return A\n",
    "\n",
    "    def _compose_arbitrary_slices(self, num_s, dimensions):\n",
    "        \"\"\"\n",
    "        Here, dimensions is the shape of the histogram\n",
    "        We want to return a list of length num_s, containing\n",
    "        random slice objects, given the dimensions\n",
    "\n",
    "        These are our linear queries\n",
    "\n",
    "        :param num_s: Number of queries (slices) to generate\n",
    "        :type num_s: int\n",
    "        :param dimensions: Dimensions of histogram to be sliced\n",
    "        :type dimensions: np.shape\n",
    "        :return: Collection of random np.s_ (linear queries) for\n",
    "        a dataset with dimensions\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        slices_list = []\n",
    "        # TODO: For analysis, generate a distribution of slice sizes,\n",
    "        # by running the list of slices on a dimensional array\n",
    "        # and plotting the bucket size\n",
    "        slices_list = []\n",
    "        for _ in range(num_s):\n",
    "            inds = []\n",
    "            for _,s in np.ndenumerate(dimensions):\n",
    "                # Random linear sample, within dimensions\n",
    "                a = np.random.randint(s)\n",
    "                b = np.random.randint(s)\n",
    "\n",
    "                l_b = min(a,b) ; u_b = max(a,b) + 1\n",
    "                pre = []\n",
    "                pre.append(l_b)\n",
    "                pre.append(u_b)\n",
    "                inds.append(pre)\n",
    "\n",
    "            # Compose slices\n",
    "            sl = []\n",
    "            for ind in inds:\n",
    "                sl.append(np.s_[ind[0]:ind[1]])\n",
    "\n",
    "            slices_list.append(sl)\n",
    "        return slices_list\n",
    "\n",
    "    def _evaluate(self, a_slice, data):\n",
    "        \"\"\"\n",
    "        Evaluate a count query i.e. an arbitrary slice\n",
    "\n",
    "        :param a_slice: Random slice within bounds of flattened data length\n",
    "        :type a_slice: np.s_\n",
    "        :param data: Data to evaluate from (synthetic dset)\n",
    "        :type data: np.ndarray\n",
    "        :return: Count from data within slice\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        # We want to count the number of objects in an\n",
    "        # arbitrary slice of our collection\n",
    "\n",
    "        # We use np.s_[arbitrary slice] as our queries\n",
    "        e = data.T[tuple(a_slice)]\n",
    "        \n",
    "        if isinstance(e, np.ndarray):\n",
    "            return np.sum(e)\n",
    "        else:\n",
    "            return e\n",
    "\n",
    "    def _binary_replace_in_place_slice(self, data, a_slice):\n",
    "        \"\"\"\n",
    "        We want to create a binary copy of the data,\n",
    "        so that we can easily perform our error multiplication\n",
    "        in MW. Convenience function.\n",
    "\n",
    "        :param data: Data\n",
    "        :type data: np.ndarray\n",
    "        :param a_slice: Slice\n",
    "        :type a_slice: np.s_\n",
    "        :return: Return data, where the range specified\n",
    "        by a_slice is all 1s.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        view = data.copy()\n",
    "        view.T[tuple(a_slice)] = 1.0\n",
    "        return view\n",
    "    \n",
    "    def _reorder(self, splits):\n",
    "        \"\"\"\n",
    "        Given an array of dimensionality splits (column indices)\n",
    "        returns the corresponding reorder array (indices to return\n",
    "        columns to original order)\n",
    "\n",
    "        Example:\n",
    "        original = [[1, 2, 3, 4, 5, 6],\n",
    "        [ 6,  7,  8,  9, 10, 11]]\n",
    "        \n",
    "        splits = [[1,3,4],[0,2,5]]\n",
    "        \n",
    "        mod_data = [[2 4 5 1 3 6]\n",
    "                [ 7  9 10  6  8 11]]\n",
    "        \n",
    "        reorder = [3 0 4 1 2 5]\n",
    "\n",
    "        :param splits: 2d list with splits (column indices)\n",
    "        :type splits: array of arrays\n",
    "        :return: 2d list with splits (column indices)\n",
    "        :rtype: array of arrays\n",
    "        \"\"\"\n",
    "        flat = np.concatenate(np.asarray(splits)).ravel()\n",
    "        reordered = np.zeros(len(flat))\n",
    "        for i, ind in enumerate(flat):\n",
    "            reordered[ind] = i\n",
    "        return reordered.astype(int)\n",
    "\n",
    "    def _generate_splits(self, n_dim, factor):\n",
    "        \"\"\"\n",
    "        If user specifies, do the work and figure out how to divide the dimensions\n",
    "        into even splits to speed up MWEM\n",
    "\n",
    "        Last split will contain leftovers <= sizeof(factor)\n",
    "\n",
    "        :param n_dim: Total # of dimensions\n",
    "        :type n_dim: int\n",
    "        :param factor: Desired size of the splits\n",
    "        :type factor: int\n",
    "        :return: Splits\n",
    "        :rtype: np.array(np.array(),...)\n",
    "        \"\"\"\n",
    "        # Columns indices\n",
    "        indices = np.arange(n_dim)\n",
    "        \n",
    "        # Split intelligently\n",
    "        fits = int((np.floor(len(indices) / factor)) * factor)\n",
    "        even_inds = indices[:fits].reshape((int(len(indices)/factor), factor))\n",
    "        s1 = even_inds.tolist()\n",
    "        if indices[fits:].size > 0: # != np.array([])\n",
    "            s1.append(indices[fits:])\n",
    "        s2 = [np.array(l) for l in s1]\n",
    "        return np.array(s2)\n",
    "\n",
    "    def _laplace(self, sigma):\n",
    "        \"\"\"\n",
    "        Laplace mechanism\n",
    "\n",
    "        :param sigma: Laplace scale param sigma\n",
    "        :type sigma: float\n",
    "        :return: Random value from laplace distribution [-1,1]\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        return sigma * np.log(random.random()) * np.random.choice([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "adult = pd.read_csv('adult.csv')\n",
    "\n",
    "adult_cat_ord = adult.copy()\n",
    "\n",
    "categorical_columns = ['workclass',\n",
    "                       'marital-status', \n",
    "                       'occupation', \n",
    "                       'relationship', \n",
    "                       'race',\n",
    "                       'gender',\n",
    "                       'native-country',\n",
    "                       'income']\n",
    "\n",
    "ordinal_columns = ['education']\n",
    "\n",
    "encoders = {}\n",
    "for column in adult.columns:\n",
    "    if adult.dtypes[column] == np.object:\n",
    "        encoders[column] = LabelEncoder()\n",
    "        adult[column] = encoders[column].fit_transform(adult[column])\n",
    "        adult_cat_ord[column] = encoders[column].fit_transform(adult[column])\n",
    "\n",
    "for c in adult_cat_ord.columns.values:\n",
    "    if not c in categorical_columns:\n",
    "        adult_cat_ord = adult_cat_ord.drop([c], axis=1)\n",
    "        \n",
    "nf = adult.to_numpy()\n",
    "nf_cat_ord = adult_cat_ord.to_numpy()\n",
    "\n",
    "\n",
    "glass = pd.read_csv('wine.csv')\n",
    "nf_g = glass.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00557933  0.02118312 -0.00341205 ... -0.00765037  0.00664334\n",
      "  0.00603217]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb103ee0940>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10000\n",
    "g_v = np.random.normal(size=n)\n",
    "print(g_v / np.linalg.norm( g_v, 2 ))\n",
    "beta = abs(np.random.normal(loc=195365.0, size=n))\n",
    "#beta = abs(np.random.normal(size=n))\n",
    "def noisevector(g_v, beta, n):    \n",
    "    gauss_vector = g_v\n",
    "    norm_vector = -1 * preprocessing.normalize([gauss_vector], norm='l2')\n",
    "    # this vector has prob density of (1.0 / beta) * np.exp(norm_vector)\n",
    "    # TODO: add test\n",
    "    noise_vector = []\n",
    "    for b in beta:\n",
    "        noise_vector = norm_vector * np.random.gamma(n, 1/b)\n",
    "    return np.array(noise_vector)[0]\n",
    "xvalues = beta\n",
    "#print(xvalues)\n",
    "n_v = noisevector(g_v, beta, n)\n",
    "yvalues1 = n_v\n",
    "yvalues2 = n_v / np.linalg.norm(n_v)\n",
    "#print(yvalues1)\n",
    "plt.scatter(xvalues, yvalues1, lw=2, color='red', label='gamma')\n",
    "# plt.scatter(xvalues, yvalues2, lw=2, color='blue', label='norm')\n",
    "plt.title('Gamma Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('gamma')\n",
    "plt.axhline(0, lw=0.5, color='black')\n",
    "plt.axvline(0, lw=0.5, color='black')\n",
    "plt.xlim((195360.0,195380.0))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from scipy.optimize import minimize\n",
    "import random\n",
    "\n",
    "def noisevector(beta,n):    \n",
    "    gauss_vector = np.random.normal(size=n)\n",
    "    norm_vector = -1 * preprocessing.normalize([gauss_vector], norm='l2')\n",
    "    # this vector has prob density of (1.0 / beta) * np.exp(norm_vector)\n",
    "    # TODO: add test\n",
    "    noise_vector = norm_vector * np.random.gamma(n, 1/beta)\n",
    "    return np.array(noise_vector[0])\n",
    "\n",
    "def lr( z ):\n",
    "\n",
    "    logr = np.log( 1 + np.exp( -z ) )\n",
    "    # print(logr)\n",
    "    return logr\n",
    "\n",
    "def lr_output_train( data, labels, epsilon, Lambda ):\n",
    "\n",
    "    L = len( labels )\n",
    "    l = len( data[0] )#length of a data point\n",
    "    scale = L * Lambda * epsilon / 2#chaudhuri2011differentially corollary 11, part 1\n",
    "    noise = noisevector( scale , l)\n",
    "    x0 = np.zeros( l )#starting point with same length as any data point\n",
    "    #print(data[0])\n",
    "    #print(labels[0])\n",
    "    def obj_func(x):\n",
    "        jfd = lr( labels[0] * np.dot( data[0] , x ) )\n",
    "        print(jfd)\n",
    "        for i in range( 1, L ):\n",
    "            jfd = jfd + lr( labels[i] * np.dot( data[i], x ) )\n",
    "        f = (1/L) * jfd + (1/2) * Lambda * ( np.linalg.norm(x)**2 )\n",
    "        #print(f)\n",
    "        return f\n",
    "    \n",
    "    #print(x0)\n",
    "    #print()\n",
    "    #minimization procedure\n",
    "    #  args=(data, labels, Lambda, L),\n",
    "    print(data.shape)\n",
    "    f = minimize( obj_f, x0, args=(data, labels, Lambda, L), method='Nelder-Mead').x#empirical risk minimization using scipy.optimize minimize function\n",
    "    fpriv = f + noise\n",
    "    return fpriv\n",
    "\n",
    "def test_dp_clf(data_tst, labels_tst, dp_clf, v=False):\n",
    "    # loop throught the data and record wrong answers\n",
    "    n = len(labels_tst)\n",
    "#     tot_err = 0\n",
    "    predictions = []\n",
    "    for i in range(n):\n",
    "        dp_ans = np.dot(data_tst[i,:], dp_clf)\n",
    "        if dp_ans >= 0.5:\n",
    "            dp_ans = 1\n",
    "        else:\n",
    "            dp_ans = 0\n",
    "        predictions.append(dp_ans)\n",
    "        # print(str(dp_ans) + ' - ' + str(labels_tst[i]))\n",
    "#         if labels_tst[i] == 0:\n",
    "#             if dp_ans >= 0.5:\n",
    "#                 tot_err += 1\n",
    "#         else:\n",
    "#             if dp_ans < .5:\n",
    "#                 tot_err += 1\n",
    "#     accuracy = (n - tot_err) * 1.0 / (n * 1.0)\n",
    "    \n",
    "    #predictions = model_real.predict(x_test)\n",
    "    accuracy = accuracy_score(labels_tst, predictions)\n",
    "    if v:\n",
    "        print()\n",
    "        print(predictions[:200])\n",
    "        print('Trained on Real Data')\n",
    "        print(classification_report(labels_tst, predictions))\n",
    "        print('Accuracy real: ' + str(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def output_perturbation(D, y, epsilon, L2):\n",
    "    n = len(y) # Number of datapoints\n",
    "    beta = (n * L2 * epsilon) / 2.0 # Beta\n",
    "    # create our noise vector (b)\n",
    "    gauss_vector = np.random.normal(size=len(D[0]))\n",
    "    norm_vector = -1 * preprocessing.normalize([gauss_vector], norm='l2')\n",
    "    # this vector has prob density of (1.0 / beta) * np.exp(norm_vector)\n",
    "    # according to 3.1 (Chaudhuri et. al)\n",
    "    # TODO: add test for this!\n",
    "    noise_vector = norm_vector * np.random.gamma(n, 1/beta)\n",
    "    noise_vector = np.array(noise_vector[0])\n",
    "    \n",
    "    # perform our ERM minimization using scipy minimizer\n",
    "    print(D.shape)\n",
    "    Nf = minimize(obj_f, np.zeros(len(D[0])), args=(D, y, L2, n), method='Nelder-Mead').x\n",
    "    \n",
    "    f_priv = Nf + noise_vector\n",
    "    return f_priv\n",
    "\n",
    "def obj_f(x, data, labels, L2, n):\n",
    "    # do first data point\n",
    "    # adjust for all datapoints\n",
    "    acc = 0\n",
    "    for i in range(n):\n",
    "        z = labels[i] * sigmoid(np.dot(data[i], x))\n",
    "        log_r = np.log(np.exp(-z) + 1)\n",
    "        acc = acc + log_r\n",
    "    Nf = (acc/n) + (1.0/L2) * (np.linalg.norm(x)**2)\n",
    "    print(Nf)\n",
    "    print(x)\n",
    "    return Nf\n",
    "\n",
    "def lr_objective_train(data, labels, epsilon, Lambda ):\n",
    "    #parameters in objective perturbation method\n",
    "    c = 1 / 4#chaudhuri2011differentially corollary 11, part 2\n",
    "    L = len( labels )#number of data points in the data set\n",
    "    l = len( data[0] )#length of a data point\n",
    "    x0 = np.zeros( l )#starting point with same length as any data point\n",
    "    Epsilonp = epsilon - 2 * np.log( 1 + c / ( Lambda * L ) )\n",
    "    if Epsilonp > 0:\n",
    "        Delta = 0\n",
    "    else:\n",
    "        Delta = c / ( L * ( np.exp( epsilon / 4 ) - 1 ) ) - Lambda\n",
    "        Epsilonp = epsilon / 2\n",
    "    scale = Epsilonp / 2\n",
    "    noise = noisevector( scale, l )\n",
    "\n",
    "    def obj_func( x ):\n",
    "        jfd = lr( labels[0] * np.dot( data[0], x ) )\n",
    "        for i in range( 1, L ):\n",
    "            jfd = jfd + lr( labels[i] * np.dot( data[i], x ) )\n",
    "        f = (1/L) * jfd + (1/2) * Lambda * ( np.linalg.norm(x)**2 ) + (1/L) * np.dot(noise, x) + (1/2) * Delta * (np.linalg.norm(x)**2)\n",
    "        return f\n",
    "\n",
    "    #minimization procedure\n",
    "    fpriv = minimize(obj_func, x0, method='Nelder-Mead').x#empirical risk minimization using scipy.optimize minimize function\n",
    "    return fpriv\n",
    "\n",
    "\n",
    "def output_perturbation_erm(D, y, epsilon, L2, div=30, model=None):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    if not model:\n",
    "        model = LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \"\"\"\n",
    "    Note that for the theoretical guarantees of DP\n",
    "    with output perturbation, as shown by Chaudhuri et. al\n",
    "    (https://arxiv.org/abs/1705.10829)\n",
    "    to hold, the model and the loss function must be 1-strongly\n",
    "    convex and differentiable. This holds for Logistic regression.\n",
    "    \n",
    "    Note that we divide our epsilon budget by div, and then by 2.\n",
    "    We run for div iterations, to attempt and output the best model\n",
    "    given our privacy constraints. There is some precedence for this\n",
    "    approach: https://arxiv.org/pdf/1705.10829.pdf\n",
    "    \n",
    "    We then perform\n",
    "    simple output perturbation for each iteration. \n",
    "    \n",
    "    We select a perturbed\n",
    "    model with the highest accuracy on our validation set. In \n",
    "    this case, the epsilon is likely additive, although a more\n",
    "    complex analysis of privacy may be required to understand the\n",
    "    true privacy guarantees.\n",
    "    \n",
    "    The current approach is to treat the accuracy comparisons as\n",
    "    counts. Each \"count\" has laplace noise added according to the\n",
    "    laplace mechanism. Note that the mechanism is parametrized\n",
    "    by 1 / count(y_test) (our test labels). This aligns with sensitivity\n",
    "    calculations returning the average value of count queries (slide 3)\n",
    "    http://cyber.biu.ac.il/wp-content/uploads/2016/09/LaplacereportNoisyMax.key.pdf\n",
    "    \n",
    "    Intuitively, you can think of the count as counting \"How many participants did\n",
    "    this model correctly label.\" We are adding noise to this value.\n",
    "    \"\"\"\n",
    "    \n",
    "    D, x_test, y, y_test = train_test_split(D, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    eps_i = epsilon / div / 2.0\n",
    "    best_model = None\n",
    "    best_model_non_priv = None\n",
    "    best_sk_priv = None\n",
    "    best_acc = 0.0\n",
    "    epsilon = eps_i\n",
    "    for e in range(div):\n",
    "        n = len(y) # Number of datapoints\n",
    "        beta = (n * L2 * epsilon) / 2.0 # Beta\n",
    "\n",
    "        gauss_vector = np.random.normal(size=len(D[0]))\n",
    "        norm_vector = -1 * preprocessing.normalize([gauss_vector], norm='l2')\n",
    "        # this vector has prob density of (1.0 / beta) * np.exp(norm_vector)\n",
    "        # which is a special case of the gamma distribution\n",
    "        # according to 3.1 (Chaudhuri et. al)\n",
    "        # TODO: add test for this!\n",
    "        noise_vector = norm_vector * np.random.gamma(n, 1/beta)\n",
    "        noise_vector = np.array(noise_vector[0])\n",
    "        \n",
    "        classifier = model(random_state = 0, C=100, solver='lbfgs', max_iter=1000, multi_class='auto') #\n",
    "        classifier.fit(D, y)\n",
    "        coef = classifier.coef_[0]\n",
    "        coef_priv = coef + noise_vector\n",
    "        \n",
    "        sk_priv_model = model()\n",
    "        sk_priv_model.classes_ = classifier.classes_\n",
    "        sk_priv_model.coef_ = np.array([coef_priv])\n",
    "        sk_priv_model.intercept_ = classifier.intercept_\n",
    "        sk_priv_model.n_iter_ = classifier.n_iter_\n",
    "        \n",
    "        # Note accuracy target epsilon work\n",
    "        # https://arxiv.org/pdf/1705.10829.pdf\n",
    "        \n",
    "        # Here we need to add noise to our accuracy score\n",
    "        # according to the laplace mechanism\n",
    "        \n",
    "        # Note: sensitivity is 1/|x| for queries returning the average\n",
    "        # value of count queries mapping X to {0,1}\n",
    "        # We are \"counting\" the number of correct answers that\n",
    "        # our model provides\n",
    "        def laplace(sigma):\n",
    "            return sigma * np.log(random.random()) * np.random.choice([-1, 1])\n",
    "        \n",
    "        # Calculate the l1 sensitivity of an accuracy measure\n",
    "        # delta f = max l1 norm(f(x) - f(y))\n",
    "        # We are evaluating our model on y_test real people,\n",
    "        # but only using the overall accuracy as a point of\n",
    "        # comparison\n",
    "        delta_f = 1 / len(y_test)\n",
    "        \n",
    "        lap = laplace(delta_f/epsilon)\n",
    "        print(lap)\n",
    "        \n",
    "        def acc(x_t, y_t, m_priv):\n",
    "            predictions = m_priv.predict(x_t)\n",
    "            return accuracy_score(y_t, predictions)\n",
    "        \n",
    "        score = acc(x_test, y_test, sk_priv_model) + lap\n",
    "        score_real = acc(x_test, y_test, sk_priv_model)\n",
    "        if score > best_acc:\n",
    "            best_acc = score\n",
    "            best_model = coef_priv\n",
    "            best_model_non_priv = classifier\n",
    "            best_sk_priv = sk_priv_model\n",
    "            \n",
    "    return best_model, best_model_non_priv, best_sk_priv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004649040454746981\n",
      "-0.0005956185154787911\n",
      "0.0009227159592042214\n",
      "0.00023728917607762157\n",
      "-0.0006515965946108453\n",
      "0.001103677090860478\n",
      "-0.0014215686866553478\n",
      "-0.0007885686083551048\n",
      "0.0011271455926245883\n",
      "-0.001134606854786345\n",
      "-0.0013047662718071668\n",
      "0.0005719685705125819\n",
      "0.0006884864918161616\n",
      "-0.0022043304837248\n",
      "2.2599799742444023e-05\n",
      "-0.0009598602109353332\n",
      "-0.0020143316850669774\n",
      "-0.003987051277733207\n",
      "-0.0024854211332219936\n",
      "-0.00018388533534747148\n",
      "0.0020184100679564053\n",
      "0.00025415874650020837\n",
      "-6.383079197608457e-05\n",
      "-0.0031494726134689453\n",
      "0.0006285218628852755\n",
      "9.52738502804241e-05\n",
      "-0.0007452433433155103\n",
      "-0.0003651124944406198\n",
      "-0.000260100578752592\n",
      "0.0005663741557521844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87      7479\n",
      "           1       0.65      0.22      0.33      2290\n",
      "\n",
      "    accuracy                           0.79      9769\n",
      "   macro avg       0.72      0.59      0.60      9769\n",
      "weighted avg       0.77      0.79      0.75      9769\n",
      "\n",
      "0.7892312416828744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87      7479\n",
      "           1       0.62      0.30      0.41      2290\n",
      "\n",
      "    accuracy                           0.79      9769\n",
      "   macro avg       0.72      0.62      0.64      9769\n",
      "weighted avg       0.77      0.79      0.77      9769\n",
      "\n",
      "0.7928140034803972\n"
     ]
    }
   ],
   "source": [
    "X = adult.iloc[:, :-1]\n",
    "y = adult.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "l , non_priv, sk_priv = output_perturbation_erm(x_train.to_numpy(), y_train.to_numpy(), 10.0, 0.1)\n",
    "\n",
    "# priv_score = test_dp_clf(x_test.to_numpy(), y_test.to_numpy(), l, v=True)\n",
    "# control_score = test_dp_clf(x_test.to_numpy(), y_test.to_numpy(), non_priv, v=True)\n",
    "# control_txt = \"Control Quality: {}\".format(round(control_score, 4))\n",
    "predictions = sk_priv.predict(x_test.to_numpy())\n",
    "print(classification_report(y_test.to_numpy(), predictions))\n",
    "print(accuracy_score(y_test.to_numpy(), predictions))\n",
    "predictions_np = non_priv.predict(x_test.to_numpy())\n",
    "print(classification_report(y_test.to_numpy(), predictions_np))\n",
    "print(accuracy_score(y_test.to_numpy(), predictions_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lurosenb/anaconda3/envs/oss_dp_2/lib/python3.8/site-packages/sklearn/linear_model/logistic.py:430: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained on Real Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88      7479\n",
      "           1       0.68      0.29      0.41      2290\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.75      0.62      0.64      9769\n",
      "weighted avg       0.78      0.80      0.77      9769\n",
      "\n",
      "Accuracy real: 0.8022315487767427\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MWEMSynthesizerTarget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-8cb2e2eca00e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mm_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msynth_cat_ord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMWEMSynthesizerTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0msynth_cat_ord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MWEMSynthesizerTarget' is not defined"
     ]
    }
   ],
   "source": [
    "# Train our seed model\n",
    "def train_seed(real, model):\n",
    "    X = real.iloc[:, :-1]\n",
    "    y = real.iloc[:, -1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model_real = model()\n",
    "    model_real.fit(x_train, y_train)\n",
    "    \n",
    "    #Test the model\n",
    "    predictions = model_real.predict(x_test)\n",
    "    print()\n",
    "    print('Trained on Real Data')\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print('Accuracy real: ' + str(accuracy_score(y_test, predictions)))\n",
    "    \n",
    "    return model_real\n",
    "\n",
    "m_r = train_seed(adult, LogisticRegression)\n",
    "\n",
    "synth_cat_ord = MWEMSynthesizerTarget(800, 10.0, 30, 20, split_factor=2)\n",
    "synth_cat_ord.fit(nf, m_r, nf.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 3999\n",
    "synthetic_cat_ord = synth_cat_ord.sample(int(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_real_vs_synthetic_data(real, synthetic, model):\n",
    "    synth_df = pd.DataFrame(synthetic, \n",
    "        index=real.index,\n",
    "        columns=real.columns)\n",
    "\n",
    "    X = real.iloc[:, :-1]\n",
    "    y = real.iloc[:, -1]\n",
    "    X_synth = synth_df.iloc[:, :-1]\n",
    "    y_synth = synth_df.iloc[:, -1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    x_train_synth, x_test_synth, y_train_synth, y_test_synth = train_test_split(X_synth, y_synth, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model_real = model()\n",
    "    model_real.fit(x_train, y_train)\n",
    "\n",
    "    model_fake = model()\n",
    "    model_fake.fit(x_train_synth, y_train_synth)\n",
    "    \n",
    "    #Test the model\n",
    "    predictions = model_real.predict(x_test)\n",
    "    print()\n",
    "    print('Trained on Real Data')\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print('Accuracy real: ' + str(accuracy_score(y_test, predictions)))\n",
    "    \n",
    "    predictions = model_fake.predict(x_test)\n",
    "    print()\n",
    "    print('Trained on Synthetic Data')\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print('Accuracy synthetic: ' + str(accuracy_score(y_test, predictions)))\n",
    "\n",
    "    # How does it compare to guessing randomly?\n",
    "    print()\n",
    "    print('Random Guessing')\n",
    "    fifty_fifty = len(y_test_synth) / 2\n",
    "    guesses = np.ones(len(y_test_synth))\n",
    "    guesses[:int(fifty_fifty)] = 0\n",
    "    np.random.shuffle(guesses)\n",
    "    print(classification_report(y_test_synth, guesses))\n",
    "    print('Accuracy guessing: ' + str(accuracy_score(y_test_synth, guesses)))\n",
    "    \n",
    "    comb = np.vstack((x_train[:1000], x_train_synth[:1000]))\n",
    "    embedding_1 = TSNE(n_components=2, perplexity=5.0, early_exaggeration=1.0).fit_transform(comb)\n",
    "    x,y = embedding_1.T\n",
    "    l = int(len(x) / 2)\n",
    "    inds = []\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "    plt.scatter(x,y,c=['purple' if i in inds else 'red' for i in range(l)]+['purple' if j in inds else 'blue' for j in range(l)])\n",
    "    plt.gca().legend(('Real Data','Real'))\n",
    "    plt.title('TSNE Plot, Real Data vs. Synthetic')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_vs_synthetic_data(adult, synthetic_cat_ord, LogisticRegression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
