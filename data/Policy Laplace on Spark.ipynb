{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Laplace on Spark\n",
    "\n",
    "Spark implementation of Policy Laplace from Differentially Private Set Union [https://arxiv.org/abs/2002.09745]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../differentially-private-set-union/data/clean_askreddit.csv\"\n",
    "reddit = spark.read.load(filepath, format=\"csv\", sep=\",\",inferSchema=\"true\", header=\"true\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Load the data from file and tokenize.  This code can be any caller-specific tokenization routine, and is independent of differential privacy.  Output RDD should include one list of tokens per row, but can have multiple rows per user, and does not need to be odered in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "n_grams = 2\n",
    "distinct = True\n",
    "\n",
    "def tokenize(user_post):\n",
    "    user, post = user_post\n",
    "    tokens = post.split(\" \")\n",
    "    if n_grams > 1:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams))\n",
    "        tokens = [\"_\".join(g) for g in tokens]\n",
    "    if distinct:\n",
    "        tokens = list(set(tokens))\n",
    "    return (user, tokens)\n",
    "        \n",
    "tokenized = reddit.select(\"author\", \"clean_text\").rdd.map(tokenize).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Delta_0=10, delta=4.54e-05, l_param=0.3333333333333333, l_rho=4.102284273146641, Gamma=5.768950939813308\n"
     ]
    }
   ],
   "source": [
    "from policy_laplace import PolicyLaplace\n",
    "\n",
    "epsilon = 3.0\n",
    "delta = np.exp(-10)\n",
    "alpha = 5.0\n",
    "tokens_per_user = 10\n",
    "\n",
    "pl = PolicyLaplace(epsilon, delta, alpha, tokens_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = pl.reservoir_sample(tokenized, distinct)\n",
    "\n",
    "ngh = sampled.repartition(1).mapPartitions(pl.process_rows).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8552 words from 554564\n"
     ]
    }
   ],
   "source": [
    "output_vocab = {}\n",
    "for ngdict in ngh:\n",
    "    for key, val in ng.items():\n",
    "        if pl.exceeds_threshold(val):\n",
    "            output_vocab[key] = val\n",
    "    print(\"Retrieved {0} words from {1}\".format(len(output_vocab),len(ngh[0].items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
