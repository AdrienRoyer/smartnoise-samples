{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries with Differentially Private Set Union (DPSU)\n",
    "\n",
    "In the context of the WhiteNoise SQL scenario, consider a guiding example: there exists a dataset with Reddit users and their posts, keeping in mind that a single user could have multiple posts. An analyst would like to release a report containing the counts of the number of bigrams per user while preserving privacy. We'd like to increase the representation of bigrams contained in this report without violating privacy bounds.\n",
    "\n",
    "You can imagine that it might be easy to identify an author's bigrams if she has only posted a few times and therefore has very few bigrams. To resolve this threat, previously, we would have to drop bigrams whose noisy counts were below a certain threshold. However, this process was wasteful because we allocated privacy budget to all rows uniformly even if their counts already exceeded the threshold.\n",
    "\n",
    "With DPSU, we resolve this issue by adding noise in a dependent fashion. This increases the dimensionality of the final dataset, and we therefore have less data loss and get a richer representation of bigrams. \n",
    "\n",
    "The code below showcases DPSU support with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from opendp.whitenoise.metadata import CollectionMetadata\n",
    "from opendp.whitenoise.metadata.collection import Table, String\n",
    "from opendp.whitenoise.sql import PrivateReader, PandasReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return input_list if n == 1 else list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def _download_file(url, local_file):\n",
    "    try:\n",
    "        from urllib import urlretrieve\n",
    "    except ImportError:\n",
    "        from urllib.request import urlretrieve\n",
    "    urlretrieve(url, local_file)\n",
    "\n",
    "reddit_url = \"https://github.com/heyyjudes/differentially-private-set-union/raw/master/data/clean_askreddit.csv.zip\"\n",
    "\n",
    "\n",
    "reddit_dataset_path = os.path.join(\"datasets\", \"reddit.csv\")\n",
    "if not os.path.exists(reddit_dataset_path):\n",
    "    reddit_zip_path = os.path.join(\"datasets\", \"askreddit.csv.zip\")\n",
    "    datasets = os.path.join(\"datasets\")\n",
    "    clean_reddit_path = os.path.join(datasets, \"clean_askreddit.csv\")\n",
    "    _download_file(reddit_url, reddit_zip_path)\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(reddit_zip_path) as zf:\n",
    "        zf.extractall(datasets)\n",
    "    reddit_df = pd.read_csv(clean_reddit_path, index_col=0)\n",
    "    reddit_df = reddit_df.sample(frac=0.05)\n",
    "    reddit_df['clean_text'] = reddit_df['clean_text'].astype(str)\n",
    "    reddit_df.loc[:,'clean_text'] = reddit_df.clean_text.apply(lambda x : str.lower(x))\n",
    "    reddit_df.loc[:,'clean_text'] = reddit_df.clean_text.apply(lambda x : \" \".join(re.findall('[\\w]+', x)))\n",
    "    reddit_df['ngram'] = reddit_df['clean_text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
    "    rows = list()\n",
    "    for row in reddit_df[['author', 'ngram']].iterrows():\n",
    "        r = row[1]\n",
    "        for ngram in r.ngram:\n",
    "            rows.append((r.author, ngram))\n",
    "    ngrams = pd.DataFrame(rows, columns=['author', 'ngram'])\n",
    "    ngrams.to_csv(reddit_dataset_path)\n",
    "\n",
    "\n",
    "reddit_schema_path = os.path.join(\"datasets\", \"reddit.yaml\")\n",
    "if not os.path.exists(reddit_schema_path):\n",
    "    reddit = Table(\"reddit\", \"reddit\", 500000, [\n",
    "                String(\"author\", card=10000, is_key=True),\n",
    "                String(\"ngram\", card=10000)\n",
    "    ])\n",
    "    schema = CollectionMetadata([reddit], \"csv\")\n",
    "    schema.to_file(reddit_schema_path, \"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207931\n"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv(\"datasets/reddit.csv\", index_col=0)\n",
    "metadata = CollectionMetadata.from_file(\"datasets/reddit.yaml\")\n",
    "\n",
    "query = \"SELECT ngram, COUNT(*) as n FROM reddit.reddit GROUP BY ngram ORDER BY n desc\"\n",
    "\n",
    "reader = PandasReader(metadata, reddit)\n",
    "exact = reader.execute(query)\n",
    "print(len(exact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ngram              |n      \n",
      " -------------------|-------\n",
      "  (n, t)            | 108   \n",
      "  (i, m)            | 78    \n",
      "  (that, s)         | 46    \n",
      "  (in, the)         | 45    \n",
      "  (of, the)         | 44    \n",
      "  (and, i)          | 39    \n",
      "  (you, re)         | 39    \n",
      "  (company, money)  | 38    \n",
      "  (i, have)         | 38    \n",
      "  (do, n)           | 37    \n",
      "  (it, s)           | 37    \n"
     ]
    }
   ],
   "source": [
    "private_reader = PrivateReader(metadata, reader)\n",
    "result = private_reader.execute_typed(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result['n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below turns off the DPSU optimization and we can see that the results of our query are worse. \n",
    "[https://arxiv.org/pdf/2002.09745.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ngram       |n      \n",
      " ------------|-------\n",
      "  (n, t)     | 113   \n",
      "  (i, m)     | 74    \n",
      "  (it, s)    | 63    \n",
      "  (do, n)    | 52    \n",
      "  (of, the)  | 43    \n",
      "  (that, s)  | 39    \n",
      "  (in, the)  | 34    \n"
     ]
    }
   ],
   "source": [
    "private_reader_korolova = PrivateReader(metadata, reader)\n",
    "private_reader_korolova.options.use_dpsu = False\n",
    "private_reader_korolova.options.max_contrib = 5\n",
    "korolova_result = private_reader_korolova.execute_typed(query)\n",
    "print(korolova_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(korolova_result['n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Module with DPSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendp.whitenoise.client import get_execution_client\n",
    "\n",
    "execution_client = get_execution_client()\n",
    "\n",
    "project = {\"params\": {\"dataset_name\": \"reddit\", \n",
    "                      \"budget\": 0.5,\n",
    "                      \"query\": \"SELECT ngram, COUNT(*) as c FROM reddit.reddit GROUP BY ngram ORDER BY c desc\"},\n",
    "           \"uri\": \"modules/sql-module\"}\n",
    "\n",
    "response = execution_client.submit(params=project[\"params\"],\n",
    "                            uri=project[\"uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = json.loads(response.result)\n",
    "pd.DataFrame.from_dict(json_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
